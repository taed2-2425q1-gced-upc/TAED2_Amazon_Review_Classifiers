{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "023d00e4",
   "metadata": {
    "id": "023d00e4"
   },
   "source": [
    "# This a sentiment analysis task that  design and optimization of the model, as well as potentially utilizing advanced NLP techniques such as fine-tuning pre-trained models like BERT. In a local Python environment where you can use TensorFlow and Keras, I took an optimized approach using a more complex LSTM model, incorporating some advanced techniques that help improve the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb52f311",
   "metadata": {
    "id": "cb52f311"
   },
   "source": [
    "## Import Libraries:\n",
    "\n",
    "#### numpy and tensorflow are foundational libraries for numerical operations and machine learning.\n",
    "#### Classes imported from tensorflow.keras are used to build neural network models:\n",
    "- Sequential for linear stacking of layers.\n",
    "- Embedding, LSTM, Dense, Dropout, and Bidirectional for different types of neural network layers.\n",
    "- Tokenizer and pad_sequences are utilities for text processing.\n",
    "- train_test_split from sklearn is used for splitting data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214a844b",
   "metadata": {
    "id": "214a844b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-04 14:36:30.231669: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-04 14:36:30.447075: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-10-04 14:36:30.447115: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-10-04 14:36:30.497265: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-04 14:36:31.364297: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-04 14:36:31.364422: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-04 14:36:31.364437: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c01d5192",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c01d5192",
    "outputId": "acdb62bd-5ff2-487a-d6d1-99aa6c1c0cc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ad5ab0",
   "metadata": {
    "id": "c7ad5ab0"
   },
   "source": [
    "## Load and Preprocess Data:\n",
    "\n",
    "### Download the NLP dataset Amazon Sentiment Review form Kaggle\n",
    "#### https://www.kaggle.com/datasets/bittlingmayer/amazonreviews\n",
    "\n",
    "- Reads each line, splits it to separate the label from the review, assigns binary labels (1 for positive, 0 for negative), and stores the results in lists.\n",
    "\n",
    "- Converts the label list to a NumPy array for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "992745e5",
   "metadata": {
    "id": "992745e5"
   },
   "outputs": [],
   "source": [
    "# Load the data (example path, replace with your actual path)\n",
    "file_path= '../data/raw/train.txt'\n",
    "\n",
    "# Preprocess the data\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "labels = []\n",
    "reviews = []\n",
    "for line in lines:\n",
    "    split_line = line.strip().split(' ', 1)\n",
    "    label = 1 if split_line[0] == '__label__2' else 0\n",
    "    review = split_line[1]\n",
    "    labels.append(label)\n",
    "    reviews.append(review)\n",
    "\n",
    "labels = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a2b587",
   "metadata": {
    "id": "f2a2b587"
   },
   "source": [
    "## Tokenization and Sequence Padding:\n",
    "\n",
    "- Initializes a Tokenizer object, specifying a maximum vocabulary size of 10,000 words and an out-of-vocabulary token <OOV>.\n",
    "- Fits the tokenizer on the collected reviews, creating an index of all unique words.\n",
    "- Converts the reviews into lists of integers based on the tokenizer's word index.\n",
    "- Pads these sequences to a fixed length of 250, ensuring all input data has consistent dimensions, necessary for training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edfcccbf",
   "metadata": {
    "id": "edfcccbf"
   },
   "outputs": [],
   "source": [
    "# Tokenizing and padding text\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "padded_sequences = pad_sequences(sequences, padding='post', maxlen=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aea0f6",
   "metadata": {
    "id": "62aea0f6"
   },
   "source": [
    "## Loading Pre-trained GloVe Embeddings\n",
    "\n",
    "### Download the Glove Embedding from Kaggle\n",
    "\n",
    "####  https://www.kaggle.com/datasets/danielwillgeorge/glove6b100dtxt\n",
    "\n",
    "- Defines a function to load the GloVe (Global Vectors for Word Representation) embeddings.\n",
    "- Reads the GloVe file, parsing each line to extract the word and its corresponding coefficient vector.\n",
    "- Creates an embedding matrix that maps each word in the tokenizer's index to its GloVe vector, if available. Words not in GloVe will have a vector of zeros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a44dac7b",
   "metadata": {
    "id": "a44dac7b"
   },
   "outputs": [],
   "source": [
    "# # Load pre-trained GloVe embeddings\n",
    "\n",
    "def load_glove_embeddings(path, word_index, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    # Open the file with UTF-8 encoding specified\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    # Prepare embedding matrix\n",
    "    num_words = min(len(word_index) + 1, 10000)\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= num_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:  # Corrected line here\n",
    "            # Use the embedding vector for words found in the GloVe index\n",
    "            embedding_matrix[i] = embedding_vector  # words not found will be all-zeros.\n",
    "\n",
    "    return embedding_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8800fc79",
   "metadata": {
    "id": "8800fc79"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Embedding/glove.6B.100d/glove.6B.100d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Size of the GloVe vectors you're using\u001b[39;00m\n\u001b[1;32m      2\u001b[0m glove_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmbedding/glove.6B.100d/glove.6B.100d.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m embedding_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mload_glove_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglove_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mload_glove_embeddings\u001b[0;34m(path, word_index, embedding_dim)\u001b[0m\n\u001b[1;32m      4\u001b[0m embeddings_index \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Open the file with UTF-8 encoding specified\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m         word, coefs \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(maxsplit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Development/taed2/TAED2_Amazon_Review_Classifiers/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Embedding/glove.6B.100d/glove.6B.100d.txt'"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100  # Size of the GloVe vectors you're using\n",
    "glove_path = 'Embedding/glove.6B.100d/glove.6B.100d.txt'\n",
    "embedding_matrix = load_glove_embeddings(glove_path, tokenizer.word_index, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed572bf",
   "metadata": {
    "id": "5ed572bf"
   },
   "source": [
    "## Model Definition and Compilation:\n",
    "\n",
    "- Defines a sequential model for sentiment analysis.\n",
    "- Adds an Embedding layer to transform indices into dense vectors of fixed size.\n",
    "- Utilizes Bidirectional layers with LSTM units to capture patterns from both forward and backward states of the input sequence.\n",
    "- Adds Dense layers with ReLU activation for non-linear transformations and a dropout layer to reduce overfitting.\n",
    "- The final output layer uses a sigmoid activation function for binary classification.\n",
    "- Compiles the model with the Adam optimizer and binary cross-entropy loss function, tracking accuracy as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b02df",
   "metadata": {
    "id": "2d8b02df"
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential([\n",
    "    Embedding(10000, embedding_dim, embeddings_initializer=Constant(embedding_matrix),\n",
    "              input_length=250, trainable=False),\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),\n",
    "    Dropout(0.5),\n",
    "    Bidirectional(LSTM(128)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4fa260",
   "metadata": {
    "id": "ce4fa260",
    "outputId": "cd4e4a54-e08e-420c-c884-95ff37da3c8c"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4e6a3",
   "metadata": {
    "id": "a3c4e6a3"
   },
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddafc948",
   "metadata": {
    "id": "ddafc948"
   },
   "source": [
    "## Model Training:\n",
    "\n",
    "- Trains the model on the padded text sequences and labels.\n",
    "- Runs for 5 epochs with 20% of the data reserved for validation to monitor performance and mitigate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596a0174",
   "metadata": {
    "id": "596a0174",
    "outputId": "bef2509a-29bd-443f-a03f-68b89959b245"
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_val, y_val), verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039f21c0",
   "metadata": {
    "id": "039f21c0"
   },
   "source": [
    "## Save the Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4693576",
   "metadata": {
    "id": "e4693576"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('../models/sentiment_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e1b099f",
   "metadata": {
    "id": "2e1b099f"
   },
   "outputs": [],
   "source": [
    "# Save the tokenizer\n",
    "import pickle\n",
    "with open('../resources/tokenizer.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66daf062",
   "metadata": {
    "id": "66daf062"
   },
   "source": [
    "## Evaluate\n",
    "\n",
    "- Evaluate the model Validation loss\n",
    "- Evaluate the model Validation accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ef635",
   "metadata": {
    "id": "b64ef635",
    "outputId": "1cc58e03-e13f-4949-eee3-f2fb57d9b846"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(\"Validation loss:\", loss)\n",
    "print(\"Validation accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7aec7",
   "metadata": {
    "id": "54a7aec7",
    "outputId": "3cb11ab3-c8c4-48ba-b50c-39c5369b2097"
   },
   "outputs": [],
   "source": [
    "#  plot the training history to visualize the learning over epochs:\n",
    "import matplotlib.pyplot as plt\n",
    "# Plotting training history\n",
    "plt.plot(history.history['accuracy'], label='Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1590396",
   "metadata": {
    "id": "f1590396"
   },
   "source": [
    "## Model Load\n",
    "\n",
    "- load the sentiment_model.h5 model from you dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57021426",
   "metadata": {
    "id": "57021426"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-04 14:56:42.736565: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-10-04 14:56:42.737484: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-10-04 14:56:42.737557: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (benji-E470): /proc/driver/nvidia/version does not exist\n",
      "2024-10-04 14:56:42.740285: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model\n",
    "model = load_model('../models/sentiment_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9acdbe1f",
   "metadata": {
    "id": "9acdbe1f"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "with open('../resources/tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b44fd582",
   "metadata": {
    "id": "b44fd582"
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf51b83",
   "metadata": {
    "id": "bbf51b83"
   },
   "source": [
    "## Utility Functions for Text Preprocessing and Sentiment Prediction:\n",
    "\n",
    "- preprocess_text converts input texts into padded sequences suitable for model input, using the previously defined tokenizer.\n",
    "- predict_sentiment processes texts, makes predictions with the trained model, and interprets the results as 'Positive' or 'Negative' based on the prediction score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de6eac0b",
   "metadata": {
    "id": "de6eac0b"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(texts, tokenizer, max_length=250):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    return padded_sequences\n",
    "\n",
    "def predict_sentiment(texts, model, tokenizer):\n",
    "    preprocessed_texts = preprocess_text(texts, tokenizer)\n",
    "    predictions = model.predict(preprocessed_texts)\n",
    "    print(\"predictions\",predictions)\n",
    "    sentiment_labels = ['Negative' if pred < 0.5 else 'Positive' for pred in predictions.flatten()]\n",
    "    return sentiment_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58a1314",
   "metadata": {
    "id": "a58a1314"
   },
   "source": [
    "## Sentiment Prediction on New Reviews:\n",
    "\n",
    "- Lists new review texts to test the model.\n",
    "- Calls predict_sentiment to determine the sentiment of each review.\n",
    "- Prints out each review with its predicted sentiment, providing a practical demonstration of the model in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d95588b0",
   "metadata": {
    "id": "d95588b0",
    "outputId": "1e924750-c0b5-49de-eeb2-127c509e2bda",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "predictions [[0.6867631 ]\n",
      " [0.6624187 ]\n",
      " [0.8604852 ]\n",
      " [0.9300392 ]\n",
      " [0.94109404]\n",
      " [0.9439397 ]\n",
      " [0.99512875]]\n",
      "Review: I absolutely loved this product, it worked wonders for me!\n",
      "Sentiment: Positive\n",
      "\n",
      "Review: Horrible experience, it broke down the first time I used it.\n",
      "Sentiment: Positive\n",
      "\n",
      "Review: Okay product, but I expected something better.\n",
      "Sentiment: Positive\n",
      "\n",
      "Review: Perfect, just as described! Would buy again!\n",
      "Sentiment: Positive\n",
      "\n",
      "Review: Not worth the money, very disappointing.\n",
      "Sentiment: Positive\n",
      "\n",
      "Review: If you want to listen to El Duke , then it is better if you have access to his shower,this is not him, it is a gimmick,very well orchestrated.\n",
      "Sentiment: Positive\n",
      "\n",
      "Review: Review of Kelly Club for Toddlers: For the price of 7.99, this PC game is WELL worth it, great graphics, colorful and lots to do! My four year old daughter is in love with the many tasks to complete in this game, including dressing and grooming wide variety of pets and decoration of numerous floats to show in your little one's very own parade.\n",
      "Sentiment: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example reviews\n",
    "new_reviews = [\n",
    "    \"I absolutely loved this product, it worked wonders for me!\",\n",
    "    \"Horrible experience, it broke down the first time I used it.\",\n",
    "    \"Okay product, but I expected something better.\",\n",
    "    \"Perfect, just as described! Would buy again!\",\n",
    "    \"Not worth the money, very disappointing.\",\n",
    "    \"If you want to listen to El Duke , then it is better if you have access to his shower,this is not him, it is a gimmick,very well orchestrated.\",\n",
    "     \"Review of Kelly Club for Toddlers: For the price of 7.99, this PC game is WELL worth it, great graphics, colorful and lots to do! My four year old daughter is in love with the many tasks to complete in this game, including dressing and grooming wide variety of pets and decoration of numerous floats to show in your little one's very own parade.\"\n",
    "]\n",
    "\n",
    "# Predict sentiments\n",
    "sentiments = predict_sentiment(new_reviews, model, tokenizer)\n",
    "# print(sentiments)\n",
    "for review, sentiment in zip(new_reviews, sentiments):\n",
    "    print(f\"Review: {review}\\nSentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a3b88ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a3b88ac",
    "outputId": "013379ac-1eff-458a-d324-4e91b6098d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 162ms/step\n",
      "Prediction: [0.96540827]\n",
      "Review: very bad product\n",
      "Sentiment: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text, tokenizer, max_length=250):\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "    return padded_sequence\n",
    "\n",
    "def predict_sentiment(text, model, tokenizer):\n",
    "    preprocessed_text = preprocess_text(text, tokenizer)\n",
    "    prediction = model.predict(preprocessed_text)[0]\n",
    "    print(\"Prediction:\", prediction)\n",
    "    sentiment_label = 'Negative' if prediction < 0.5 else 'Positive'\n",
    "    return sentiment_label\n",
    "\n",
    "# Example review\n",
    "new_review = \"very bad product\"\n",
    "\n",
    "# Predict sentiment\n",
    "sentiment = predict_sentiment(new_review, model, tokenizer)\n",
    "print(f\"Review: {new_review}\\nSentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35e0602",
   "metadata": {
    "id": "f35e0602"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
